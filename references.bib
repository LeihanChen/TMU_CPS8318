@article{chawla2002smote,
  title   = {SMOTE: synthetic minority over-sampling technique},
  journal = {Journal of artificial intelligence research},
  volume  = {16},
  pages   = {321--357},
  year    = {2002}
}

@misc{dry_bean_602,
  title        = {{Dry Bean}},
  year         = {2020},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C50S4B}
}

# give me a citation of URL https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation
@misc{iris_53,
  author       = {Fisher, R. A.},
  title        = {{Iris}},
  year         = {1936},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C56C76}
}

@article{MACKIEWICZ1993303,
  title    = {Principal components analysis (PCA)},
  journal  = {Computers & Geosciences},
  volume   = {19},
  number   = {3},
  pages    = {303-342},
  year     = {1993},
  issn     = {0098-3004},
  author   = {Andrzej Maćkiewicz and Waldemar Ratajczak},
  keywords = {Principal Components Analysis, Variance-covariance matrix, Coefficients of determination, Eigenvalues, Eigenvectors, Correlation matrix, Bartlett's statistics, FORTRAN 77},
  abstract = {Principal Components Analysis (PCA) as a method of multivariate statistics was created before the Second World War. However, the wider application of this method only occurred in the 1960s, during the “Quantitative Revolution” in the Natural and Social Sciences. The main reason for this time-lag was the huge difficulty posed by calculations involving this method. Only with the advent and development of computers did the almost unlimited application of multivariate statistical methods, including principal components, become possible. At the same time, requirements arose for precise numerical methods concerning, among other things, the calculation of eigenvalues and eigenvectors, because the application of principal components to technical problems required absolute accuracy. On the other hand, numerous applications in Social Sciences gave rise to a significant increase in the ability to interpret these nonobservable variables, which is just what the principal components are. In the application of principal components, the problem is not only to do with their formal properties but above all, their empirical origins. The authors considered these two tendencies during the creation of the program for principal components. This program—entitled PCA—accompanies this paper. It analyzes consecutively, matrices of variance-covariance and correlations, and performs the following functions: •- the determination of eigenvalues and eigenvectors of these matrices.•- the testing of principal components.•- the calculation of coefficients of determination between selected components and the initial variables, and the testing of these coefficients,•- the determination of the share of variation of all the initial variables in the variation of particular components,•- construction of a dendrite for the initial set of variables,•- the construction of a dendrite for a selected pattern of the principal components,•- the scatter of the objects studied in a selected coordinate system. Thus, the PCA program performs many more functions especially in testing and graphics, than PCA programs in conventional statistical packages. Included in this paper are a theoretical description of principal components, the basic rules for their interpretation and also statistical testing.}
}

# create citations of OneVsRestClassifier and OneVsOneClassifier from scikit-learn url
@online{scikit-learn_cross_validation,
  author       = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, É.},
  title        = {{Cross-validation: evaluating estimator performance}},
  year         = {2013},
  howpublished = {scikit-learn: machine learning in Python},
  url          = {https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation},
  urldate      = {2024-09-25}
}

@online{scikit-learn_LDA,
  author       = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, É.},
  title        = {{Linear Discriminant Analysis}},
  year         = {2013},
  howpublished = {scikit-learn: machine learning in Python},
  url          = {https://scikit-learn.org/dev/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html},
  urldate      = {2024-10-09}
}

# SMOTE citation
@online{scikit-learn_LogisticRegression,
  author       = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, É.},
  title        = {{Logistic Regression}},
  year         = {2013},
  howpublished = {scikit-learn: machine learning in Python},
  url          = {https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html},
  urldate      = {2024-10-09}
}

# LDA scikit learn citation
@online{scikit-learn_OneVsOneClassifier,
  author       = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, É.},
  title        = {{OneVsOneClassifier}},
  year         = {2013},
  howpublished = {scikit-learn: machine learning in Python},
  url          = {https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html},
  urldate      = {2024-10-09}
}

# Logistic Regression scikit learn citation
@online{scikit-learn_OneVsRestClassifier,
  author       = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, É.},
  title        = {{OneVsRestClassifier}},
  year         = {2013},
  howpublished = {scikit-learn: machine learning in Python},
  url          = {https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html},
  urldate      = {2024-10-09}
}

# citation for sklearn API
@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
                Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
                Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
                and Jaques Grobler and Robert Layton and Jake VanderPlas and
                Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
                project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}